---
title: "Latent Dirichlet Allocation of KAP Open Ended Responses"
output: html_notebook
---



```{r}
library(tm)
library(topicmodels)
library(tidyverse)
library(tidytext)
library(slam)

```

```{r}
importdf = read.csv('dftxt.csv', header = F, stringsAsFactors = F)
```

```{r}
import_corpus = Corpus(VectorSource(importdf))
```

```{r}
import_mat = 
  DocumentTermMatrix(import_corpus,
           control = list(stemming = TRUE, #create root words
                          stopwords = TRUE, #remove stop words
                          minWordLength = 3, #cut out small words
                          removeNumbers = TRUE, #take out the numbers
                          removePunctuation = TRUE)) #take out punctuation 
```

```{r}
import_weight = tapply(import_mat$v/row_sums(import_mat)[import_mat$i], 
                       import_mat$j, 
                       mean) *
  log2(nDocs(import_mat)/col_sums(import_mat > 0))

#ignore very frequent and 0 terms
import_mat = import_mat[ , import_weight >= .1]
import_mat = import_mat[ row_sums(import_mat) > 0, ]
```

```{r}
k = 5 #set the number of topics

SEED = 2010 #set a random number 

LDA_fit = LDA(import_mat, k = k, 
              control = list(seed = SEED))

LDA_fixed = LDA(import_mat, k = k, 
                control = list(estimate.alpha = FALSE, seed = SEED))
```

```{r}
LDA_gibbs = LDA(import_mat, k = k, method = "Gibbs", 
                control = list(seed = SEED, burnin = 1000, 
                               thin = 100, iter = 1000))

CTM_fit = CTM(import_mat, k = k, 
              control = list(seed = SEED, 
                             var = list(tol = 10^-4), 
                             em = list(tol = 10^-3)))
```
```{r}
LDA_fit@alpha
```

```{r}
LDA_fixed@alpha
```

```{r}
LDA_gibbs@alpha
```

```{r}
sapply(list(LDA_fit, LDA_fixed, LDA_gibbs, CTM_fit), 
       function (x) 
         mean(apply(posterior(x)$topics, 1, function(z) - sum(z * log(z)))))
```
```{r}
topics(LDA_fit, k)
```

```{r}
terms(LDA_fit,5)
```
```{r}
terms(LDA_gibbs,5)
```
```{r}
#use tidyverse to clean up the the fit     
LDA_fit_topics = tidy(LDA_fit, matrix = "beta")

#create a top terms 
top_terms = LDA_fit_topics %>%
   group_by(topic) %>%
   top_n(10, beta) %>%
   ungroup() %>%
   arrange(topic, -beta)
```

```{r}
cleanup = theme(panel.grid.major = element_blank(), 
                panel.grid.minor = element_blank(), 
                panel.background = element_blank(), 
                axis.line.x = element_line(color = "black"),
                axis.line.y = element_line(color = "black"),
                legend.key = element_rect(fill = "white"),
                text = element_text(size = 10))
```

```{r}
#make the plot
top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  cleanup +
  coord_flip()
```
```{r}
LDA_gamma = tidy(LDA_fit, matrix = "gamma")

LDA_gamma %>%
  ggplot(aes(factor(topic), gamma)) +
  geom_point() + 
  cleanup
```

```{r}
library(tm)

```

```{r}
##r chunk
#Lower case all words
set.seed(123)
import_corpus = tm_map(import_corpus, tolower) 

#Remove punctuation for creating spaces
import_corpus = tm_map(import_corpus, removePunctuation) 

#Remove stop words
import_corpus = tm_map(import_corpus, 
                     function(x) removeWords(x, stopwords("english")))
```
```{r}
import_matrix=as.matrix(TermDocumentMatrix(import_corpus))
```

```{r}
library(lsa)

#Weight the semantic space
book_weight = lw_logtf(import_matrix) * gw_idf(import_matrix)
```
```{r}
book_lsa = lsa(book_weight)

#Convert to textmatrix for coherence
book_lsa = as.textmatrix(book_lsa)
```
```{r}
library(LSAfun, quietly = T)
```

```{r}
plot_neighbors("bat", #single word
               n = 10, #number of neighbors
               tvectors = book_lsa, #matrix space
               method = "MDS", #PCA or MDS
               dims = 2) #number of dimensions
```
```{r}
summary(import_corpus)
```
```{r}
import_matrix=as.matrix(TermDocumentMatrix(import_corpus))
import_matrix.freq<-sort(rowSums(import_matrix), decreasing=TRUE)
head(import_matrix.freq)##what are the top words?

```

```{r}
library(wordcloud)
```

```{r}
set.seed(32) #be sure to set the seed if you want to reproduce the same again

wordcloud(words=names(import_matrix.freq), freq=import_matrix.freq, scale=c(3,.5),max.words = 100, random.order = TRUE)
```

```{r}
library(wesanderson)

wordcloud(words=names(import_matrix.freq), freq=import_matrix.freq, scale=c(4,.3),max.words = 100, 
          random.order = TRUE, color=wes_palette("Darjeeling1"))
```

```{r}
wordcloud(words=names(import_matrix.freq), import_matrix.freq, scale=c(4,.3),max.words = 
            150, 
          random.order = FALSE, color=wes_palette("Darjeeling1"),rot.per=.7)
```

